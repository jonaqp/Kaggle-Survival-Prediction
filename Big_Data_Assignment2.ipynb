{"cells":[{"cell_type":"code","source":["import pandas as pd\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.functions import Column\nfrom pyspark.sql.types import *\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\ntrain_data = sqlContext.read.load('/FileStore/tables/train.csv', \n                          format='com.databricks.spark.csv', \n                          header='true',  \n                          inferSchema='true')\n\noriginal_test_data = sqlContext.read.load('/FileStore/tables/test.csv', \n                          format='com.databricks.spark.csv', \n                          header='true', \n                          inferSchema='true')\n\ntest_data_survived = original_test_data.withColumn(\"Survived\", lit('none'))\n\n#test_data_survived.show(419)\n#train_data.show(891)\n\n#train_data.select('Name').distinct().count()\n#test_data_survived.select('Name').distinct().count()\n\n'''preprocessing'''\n'''removing title from names and creating title column'''\nname_with_title = train_data.select(split('Name', ',')[1].alias('Name'))\nonly_title = name_with_title.select(split('Name', \" \")[1].alias('title'))\n \nonly_title = only_title.withColumn(\"row_id\", monotonically_increasing_id())\ntrain_data = train_data.withColumn(\"row_id\", monotonically_increasing_id())\ntrain_data = train_data.join(only_title, train_data.row_id == only_title.row_id).drop(\"row_id\")\n#train_data.select('title').distinct().show()\n\n'''grouping title into 4 categories as Sir, Lady, Boy & girl'''\nupdated_train_data = train_data.withColumn(\"title\", regexp_replace(\"title\", \"Miss.\", \"girl\"))\nupdated_train_data1 = updated_train_data.withColumn(\"title\", regexp_replace(\"title\", \"Ms.\", \"girl\"))\nupdated_train_data2 = updated_train_data1.withColumn(\"title\", regexp_replace(\"title\", \"Mme.\", \"Lady\"))\nupdated_train_data3 = updated_train_data2.withColumn(\"title\", regexp_replace(\"title\", \"Mlle.\", \"Lady\"))\nupdated_train_data4 = updated_train_data3.withColumn(\"title\", regexp_replace(\"title\", \"Dona.\", \"Lady\"))\nupdated_train_data5 = updated_train_data4.withColumn(\"title\", regexp_replace(\"title\", \"Lady.\", \"Lady\"))\nupdated_train_data6 = updated_train_data5.withColumn(\"title\", regexp_replace(\"title\", \"the\", \"Lady\"))\nupdated_train_data7 = updated_train_data6.withColumn(\"title\", regexp_replace(\"title\", \"Capt.\", \"Sir\"))\nupdated_train_data8 = updated_train_data7.withColumn(\"title\", regexp_replace(\"title\", \"Don.\", \"Sir\"))\nupdated_train_data9 = updated_train_data8.withColumn(\"title\", regexp_replace(\"title\", \"Major.\", \"Sir\"))\nupdated_train_data10 = updated_train_data9.withColumn(\"title\", regexp_replace(\"title\", \"Sir.\", \"Sir\"))\nupdated_train_data11 = updated_train_data10.withColumn(\"title\", regexp_replace(\"title\", \"Jonkheer.\", \"Sir\"))\nupdated_train_data12 = updated_train_data11.withColumn(\"title\", regexp_replace(\"title\", \"Col.\", \"Sir\"))\nupdated_train_data13 = updated_train_data12.withColumn(\"title\", regexp_replace(\"title\", \"Rev.\", \"Sir\"))\nupdated_train_data14 = updated_train_data13.withColumn(\"title\", regexp_replace(\"title\", \"Mrs.\", \"Lady\"))\nupdated_train_data15 = updated_train_data14.withColumn(\"title\", regexp_replace(\"title\", \"Mr.\", \"Sir\"))\nupdated_train_data16 = updated_train_data15.withColumn(\"title\", regexp_replace(\"title\", \"Master.\", \"Boy\"))\nupdated_train_data17 = updated_train_data16.withColumn(\"title\",when((updated_train_data16.Sex == 'male') & \\\n                                                                 (updated_train_data16.title == 'Dr.'), 'Sir').otherwise(updated_train_data16.title))\ntrain_data_withtitle = updated_train_data17.withColumn(\"title\",when((updated_train_data17.Sex == 'female') & \\\n                                                                 (updated_train_data17.title == 'Dr.'), 'Lady').otherwise(updated_train_data17.title))\n\n'''finding family size for each name'''\ndef add(x,y):\n    return x+y+1\n  \ncolumn_add = udf(add, IntegerType())\ntrain_data_withfamily = train_data_withtitle.withColumn(\"familysize\", column_add('SibSP', 'Parch')) #use SibSp values & Parch + 1 to find family size\n#mean_fare = train_data_withfamily.select(avg(\"Fare\")).show()\n\n'''handling na's'''\ntrain_data_fare = train_data_withfamily.withColumn(\"Fare\", when((train_data_withfamily.Fare == 0.0), 32.2042079685746).otherwise\\\n                                                   (train_data_withfamily.Fare))\ntrain_data_fare = train_data_fare.withColumn(\"Age\", when((train_data_fare.Age.isNull()),29.69911764705882).otherwise(train_data_fare.Age))\n#train_data_fare.select(\"Fare\").distinct().show()\n#train_data_fare.show()\n#train_data.select(avg(\"Age\")).show()\n#train_data_fare.select(\"Age\").fillna(29.69911764705882)\n#train_data_fare.fillna({'Age':29.69911764705882})\n#train_data_fare.select(\"Age\").show()\n\n'''\ntrain_data_fare.select('Pclass').printSchema()\ntrain_data_fare.select('title').printSchema()\ntrain_data_fare.select('Fare').printSchema()\ntrain_data_fare.select('Age').printSchema()\ntrain_data_fare.select('Sex').printSchema()\ntrain_data_fare.select('familysize').printSchema()\ntrain_data_fare.select('familysize').distinct().show()\n'''\n\n'''String Indexer, One Hot Encoder & Vector Assembler'''\nstringIndexer = StringIndexer(inputCol=\"title\", outputCol=\"titleIndex\")\nmodel = stringIndexer.fit(train_data_fare) \nindexed = model.transform(train_data_fare)\n\nstringIndexer1 = StringIndexer(inputCol=\"Sex\", outputCol=\"Sexindex\")\nmodel1 = stringIndexer1.fit(indexed)\nindexed1 = model1.transform(indexed)\n\nencoder = OneHotEncoder(inputCol=\"titleIndex\", outputCol=\"titleVec\")\nencoded = encoder.transform(indexed1)\nencoder1 = OneHotEncoder(inputCol=\"Sexindex\", outputCol=\"SexVec\")\nencoded1 = encoder1.transform(encoded)\n\n#encoded1.select(\"titleIndex\").show()\n#encoded1.select(\"Sexindex\").show()\n#encoded1.select(\"titleVec\").show()\n#encoded1.select(\"SexVec\").show()\n\n#encoded1.show()\nassembler = VectorAssembler(inputCols=[\"titleVec\",\"SexVec\",\"Pclass\",\"Fare\",\"familysize\",\"Age\"], outputCol = \"features\")\ntrain_features = assembler.transform(encoded1)\ntrain_features.select(\"features\").show(truncate = False)\n\n#encoded1.where(encoded1.titleVec.isNull()).count()\n#encoded1.where(encoded1.SexVec.isNull()).count()\n#encoded1.where(encoded1.Pclass.isNull()).count()\n#encoded1.where(encoded1.Fare.isNull()).count()\n#encoded1.where(encoded1.familysize.isNull()).count()\n#encoded1.where(encoded1.Age.isNull()).count()\n\n\n#SibSp_rdd = train_data_withtitle.select(\"SibSp\").rdd\n#Parch_rdd = train_data_withtitle.select(\"Parch\").rdd\n#family_size_rdd = SibSp_rdd.zip(Parch_rdd).map( lambda x : int(x))\n#family_size_rdd.take(3)\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["'''model building, random forest with tuning'''\n\n'''splitting training data into train & test'''\n(training_data, test_data) = train_features.randomSplit([0.7, 0.3], seed = 100)\ntraining_data.count()\ntest_data.count()\n\nrf = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\")\nrfModel = rf.fit(training_data)\n\nrf_predictions = rfModel.transform(test_data)\n#rf_predictions.select(\"Survived\", \"prediction\", \"probability\", \"title\",\"Sex\",\"Pclass\",\"Fare\",\"familysize\",\"Age\", \"features\").show(truncate=False)\n#x = rf_predictions.toPandas()\n#print(x)\nevaluator = MulticlassClassificationEvaluator().setLabelCol(\"Survived\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n#evaluator1 = BinaryClassificationEvaluator()\nevaluator.evaluate(rf_predictions) #0.7870036101083032\n#evaluator1.evaluate(rf_predictions) "],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["'''rf tuning'''\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder().addGrid(rf.maxDepth, [2, 4, 6])\\\n            .addGrid(rf.maxBins, [20, 60])\\\n            .addGrid(rf.numTrees, [5, 20])\\\n            .build())\n\ncv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\nrfcv_model = cv.fit(training_data)\nrf_cvPredictions = rfcv_model.transform(test_data)\nevaluator.evaluate(rf_cvPredictions) #0.8194945848375451\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["'''Decision Tree'''\nfrom pyspark.ml.classification import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(labelCol=\"Survived\", featuresCol=\"features\", maxDepth=3)\ndt_model = dt.fit(training_data)\ndt_predictions = dt_model.transform(test_data)\nevaluator.evaluate(dt_predictions) #0.8194945848375451\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["'''dt tuning'''\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\\\n             .addGrid(dt.maxDepth, [1,2,6,10])\\\n             .addGrid(dt.maxBins, [20,40,80])\\\n            .build())\n\ncv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\ndt_cvmodel = cv.fit(training_data)\ndt_cvpredictions = dt_cvmodel.transform(test_data)\nevaluator.evaluate(dt_cvpredictions) #0.8267148014440433"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["cv_model.bestModel.numNodes\ncv_model.bestModel.depth"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["'''Logistic Regression'''\nfrom pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(labelCol=\"Survived\", featuresCol=\"features\", maxIter=10)\nlr_model = lr.fit(training_data)\nlr_predictions = lr_model.transform(test_data)\nevaluator.evaluate(lr_predictions) #0.7870036101083032\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["'''lr tuning'''\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\\\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\\\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n             .addGrid(lr.maxIter, [1, 5, 10])\\\n             .build())\n\nlr_cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n\nlr_cvModel = cv.fit(training_data)\n\nlr_cvpredictions = lr_cvModel.transform(test_data)\nevaluator.evaluate(lr_cvpredictions) #0.8267148014440433"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["'''Multilayer perceptron'''\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier\n\n(train, test) = train_features.randomSplit([0.6, 0.4], 1234)\n\nlayers = [len(\"features\"), 20, 10, 2]\ntrain_mt = MultilayerPerceptronClassifier(labelCol=\"Survived\",featuresCol=\"features\",maxIter=100, layers=layers, blockSize=128, seed=1234)\\\n\nmt_model = train_mt.fit(train)\nmt_predictions = mt_model.transform(test)\npredictions_and_labels = mt_predictions.select(\"prediction\", \"Survived\")\nevaluator.evaluate(predictions_and_labels) #~ 0.75 - 0.78"],"metadata":{},"outputs":[],"execution_count":9}],"metadata":{"name":"Big_Data_Assignment2","notebookId":1304428475110600},"nbformat":4,"nbformat_minor":0}
